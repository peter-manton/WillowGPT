# ğŸš€ Post-Training Quantization for Hugging Face Models

This repository contains a **Post-Training Quantization (PTQ) script** for Hugging Face Transformer models. The script converts large models to **FP16 (Half Precision)** format for **GPU acceleration**, improving inference speed while reducing model size.

---

## ğŸ“Œ Features
- âœ… **Supports GPU Acceleration (CUDA)**
- âœ… **FP16 Quantization (Half Precision) for Faster Inference**
- âœ… **Reduces Model Size and Improves Speed**
- âœ… **CLI Support - Specify Model Path Dynamically**
- âœ… **Compatible with Hugging Face Transformers**

---

## âš™ï¸ Installation
Ensure you have the necessary dependencies installed:
```bash
pip install torch torchvision transformers
```

---

## ğŸ“œ Usage
Run the quantization script by providing the **model directory path**:
```bash
python post_training_quantization.py /path/to/your/model/directory
```

### **Example Output**
```bash
Loading model from: orig_wind_in_the_willows_model
Original Model Size: 474.75 MB
Quantized Model Size: 237.37 MB
Model Size Reduction: 50.00%
Inference Time (Original Model): 0.0376 sec
Inference Time (Quantized Model): 0.0192 sec
Inference Speed-up: 48.94%
```

---

## ğŸ“Š Quantization Overview

### **What is Quantization?**
Quantization reduces the **precision** of model weights from high-precision floating-point numbers (**FP32**) to lower-precision formats (**FP16, INT8, or INT4**), which decreases memory usage and speeds up inference.

There are **two primary types of quantization**:
1. **Post-Training Quantization (PTQ)** ğŸ› ï¸
2. **Quantization-Aware Training (QAT) - Pre-Training Quantization** ğŸ¯

---

## ğŸ”„ Post-Training Quantization (PTQ)
### âœ… **Pros**
- **No Retraining Required** â€“ Faster and easier to apply.
- **Reduces Inference Latency** â€“ Models run significantly faster on GPUs and CPUs.
- **Lower Memory Usage** â€“ Reduces VRAM and RAM consumption.
- **Improves Deployment Efficiency** â€“ Makes large LLMs deployable on edge devices.

### âŒ **Cons**
- **Slight Accuracy Drop** â€“ Small performance degradation (typically 1-2%).
- **Limited to Weight Compression** â€“ Does not optimize activations like QAT.
- **May Not Work for All Layers** â€“ Some architectures (e.g., `nn.Embedding`) require special handling.

---

## ğŸ¯ Quantization-Aware Training (QAT) â€“ Pre-Training Quantization
### âœ… **Pros**
- **Minimizes Accuracy Loss** â€“ Model is trained while considering quantization.
- **Optimized for INT8 Deployments** â€“ Results in highly optimized INT8 models.
- **Best for Edge Devices** â€“ Useful for mobile inference (TensorRT, ONNX, TFLite).

### âŒ **Cons**
- **Requires Retraining** â€“ Needs access to dataset & compute resources.
- **Takes Longer** â€“ Must retrain the model for multiple epochs.
- **More Complex** â€“ Requires model modifications during training.

---

## âš–ï¸ **PTQ vs. QAT: When to Use What?**
| Use Case | Best Quantization Method |
|----------|--------------------------|
| Faster inference on GPUs | **PTQ (FP16/INT8)** |
| Smaller model size for deployment | **PTQ (INT8)** |
| Maximum accuracy with quantization | **QAT** |
| Running models on edge devices (mobile) | **QAT (TensorRT, TFLite)** |
| No access to training data | **PTQ** |