# ğŸš€ GPT-2 Model Distillation for "Wind in The Willows" LLM

This project distills a fine-tuned **GPT-2 model** into a smaller, more efficient version using **knowledge distillation**. Distillation reduces the model size while maintaining its performance, making it faster and more suitable for deployment.

## ğŸ“– What is Knowledge Distillation?
**Knowledge Distillation (KD)** is a model compression technique where a large, pretrained model (**teacher model**) transfers its knowledge to a smaller, more efficient model (**student model**).

### ğŸ”¬ **How It Works**
1. **Teacher Model** (fine-tuned GPT-2) generates outputs (logits).
2. **Student Model** (smaller GPT-2) is trained to mimic the **soft labels** (probability distributions) of the teacher.
3. The student learns from the teacherâ€™s predictions instead of just raw labels, allowing it to generalize better.

---

## âœ… **Pros & Cons of Distillation**

### âœ”ï¸ **Pros**
- ğŸ”¥ **Faster inference** â†’ Smaller model = lower latency.
- ğŸ’¾ **Lower memory usage** â†’ Suitable for edge devices.
- ğŸ¯ **Maintains accuracy** â†’ Retains most of the original modelâ€™s knowledge.
- âš¡ **Efficient deployment** â†’ Can run on CPUs and low-power GPUs.

### âŒ **Cons**
- ğŸš€ **Extra training required** â†’ Training the student model takes additional compute.
- ğŸ­ **Potential accuracy loss** â†’ Student may not fully match the teacherâ€™s performance.
- ğŸ›ï¸ **Hyperparameter tuning needed** â†’ Requires careful optimization for best results.

---

## ğŸ› ï¸ **Setup Instructions**
### **1ï¸âƒ£ Install Dependencies**
Ensure you have Python and the required libraries installed:

```bash
pip install torch transformers datasets argparse
```

---

### **2ï¸âƒ£ Run the Distillation Script**
The script requires three inputs:
1. **Path to the fine-tuned model (`--model_dir`)**
2. **Path to the dataset (`--dataset_path`)**
3. **Path to save the distilled model (`--output_dir`)**

Run the script with:
```bash
python model_distillation.py --model_dir ./wind_in_the_willows_model --dataset_path wind_in_the_willows.txt --output_dir ./wind_in_the_willows_distilled
```

---

## âš™ï¸ **Script Breakdown**
- **Loads a fine-tuned GPT-2 model (teacher)**
- **Loads a smaller GPT-2 model (student)**
- **Tokenizes the dataset**
- **Trains the student to mimic the teacher's output**
- **Saves the distilled model to the specified directory**

---

## ğŸ¯ **Expected Output**
After training, you should see something like:
```
ğŸ” Loading teacher model from: ./wind_in_the_willows_model
ğŸ” Loading student model (distilgpt2)...
ğŸ“š Loading dataset from: wind_in_the_willows.txt
âš™ï¸ Tokenizing dataset...
ğŸš€ Starting distillation training...
âš ï¸ Skipping empty input batch...
âœ… Epoch 1: Average Loss 0.5672
âœ… Epoch 2: Average Loss 0.4238
âœ… Epoch 3: Average Loss 0.3210
ğŸ’¾ Saving distilled student model to: ./wind_in_the_willows_distilled
âœ… Distilled model saved successfully!
```

---

## ğŸ“œ **References**
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [Knowledge Distillation Paper](https://arxiv.org/abs/1503.02531)